## Computational Narratology

### Complex Event Processing

How might the processing and comprehension of stories' [fabulae](https://en.wikipedia.org/wiki/Fabula_and_syuzhet) relate to other existing techniques for processing and analyzing various other kinds of chronologically-sorted sequences of events, e.g., [complex event processing](https://en.wikipedia.org/wiki/Complex_event_processing)?

In these regards, extensible [knowledge-graph](https://en.wikipedia.org/wiki/Knowledge_graph) approaches to representing events and processing such streams of events are of interest.

### Complex Stateful Systems

Representing systems' [states](https://en.wikipedia.org/wiki/State_(computer_science)) is of fundamental importance to computer science. Contemporary possibilities, in these regards, include uses of [graph nodes](https://en.wikipedia.org/wiki/Vertex_(graph_theory)), [knowledge graphs](https://en.wikipedia.org/wiki/Knowledge_graph), and [embedding vectors](https://en.wikipedia.org/wiki/Embedding_(machine_learning)).

Considered, here, are sets of intercommunicating stateful systems, e.g., [finite-state machines](https://en.wikipedia.org/wiki/Finite-state_machine) and [automata](https://en.wikipedia.org/wiki/Automata_theory), and how these techniques, involving concurrency, can be brought to bear to address the challenges of computationally processing and comprehending stories.

Will artificial-intelligence systems processing incrementally-arriving story events represent their state progressions as simple sequences of graph nodes, knowledge graphs, or embedding vectors? Might, instead, systems' states be composite, each state containing multiple graph nodes, knowledge graphs, or embedding vectors? Regardless of whether systems' states are simple or composite, as systems' states are progressed through as a result of processing incoming story events, how might deltas or [differences](https://en.wikipedia.org/wiki/Data_differencing) between consecutive states be obtained and analyzed?

### Automata Learning and Grammar Induction

#### Recurrent Neural Networks

_Coming soon._

* Aichernig, Bernhard K., Sandra KÃ¶nig, Cristinel Mateis, Andrea Pferscher, and Martin Tappler. "Learning minimal automata with recurrent neural networks." _Software and Systems Modeling_ 23, no. 3 (2024): 625-655.

* Giles, C. Lee, Clifford B. Miller, Dong Chen, Hsing-Hen Chen, Guo-Zheng Sun, and Yee-Chun Lee. "Learning and extracting finite state automata with second-order recurrent neural networks." _Neural Computation_ 4, no. 3 (1992): 393-405.

### Automata and Large Language Models

Beyond performing symbol-matching or object-processing on automata edges, that some edges could consult LLMs, e.g., with (e.g., Boolean) natural-language questions about complex inputs.

State-based approaches could simplify managing those tools instantaneously available to LLMs. The tools instantaneously available to an LLM could be those tools referenced by the one or more nodes together representing applications' or systems' states.

### Generative and Agentic Narratology

_Coming soon._
